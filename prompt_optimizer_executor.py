import json
import logging
from typing import Dict, Any, Optional
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.events import EventQueue
from a2a.utils import new_agent_text_message
from prompt_optimizer import PromptOptimizerWorkflow, PromptRequest

# è®¾ç½®æ—¥å¿—è®°å½•
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class PromptOptimizerAgentExecutor(AgentExecutor):
    """Promptä¼˜åŒ–å™¨Agentæ‰§è¡Œå™¨ï¼Œé›†æˆå¤šAgentå·¥ä½œæµåˆ°A2Aæ¡†æ¶"""

    def __init__(self):
        # ä¸å†é¢„å…ˆåˆ›å»ºworkflowï¼Œè€Œæ˜¯åœ¨æ‰§è¡Œæ—¶æ ¹æ®æ¨¡å‹ç±»å‹åŠ¨æ€åˆ›å»º
        self._workflows = {}  # ç¼“å­˜ä¸åŒæ¨¡å‹ç±»å‹çš„workflow

    async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:
        """æ‰§è¡Œpromptä¼˜åŒ–æµç¨‹"""
        try:
            # è§£æè¾“å…¥æ¶ˆæ¯ - ä½¿ç”¨æ­£ç¡®çš„A2Aæ–¹æ³•
            user_input = context.get_user_input()
            
            if not user_input:
                await self._send_error_message(event_queue, "è¯·æä¾›éœ€è¦ä¼˜åŒ–çš„promptä¿¡æ¯")
                return

            # å°è¯•è§£æJSONæ ¼å¼çš„è¾“å…¥
            request_data = self._parse_input(user_input)
            
            if not request_data:
                self._send_usage_help(event_queue)
                return

            # éªŒè¯è¾“å…¥æ•°æ®
            validation_error = self._validate_request_data(request_data)
            if validation_error:
                await self._send_error_message(event_queue, validation_error)
                return

            # æå–æ¨¡å‹ç±»å‹ï¼Œé»˜è®¤ä¸ºgemini
            model_type = request_data.get("model_type", "gemini").lower()
            
            # éªŒè¯æ¨¡å‹ç±»å‹
            if model_type not in ["gemini", "openai"]:
                await self._send_error_message(
                    event_queue, 
                    f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}ã€‚æ”¯æŒçš„ç±»å‹: gemini, openai"
                )
                return

            # åˆ›å»ºpromptè¯·æ±‚
            prompt_request = PromptRequest(
                role=request_data.get("role", "general user"),
                examples=request_data.get("examples", []),
                additional_requirements=request_data.get("additional_requirements", ""),
                model_type=model_type
            )

            # å‘é€å¼€å§‹å¤„ç†çš„æ¶ˆæ¯
            event_queue.enqueue_event(
                new_agent_text_message(f"ğŸš€ å¼€å§‹ä¸º '{prompt_request.role}' ä½¿ç”¨ {model_type.upper()} æ¨¡å‹ä¼˜åŒ–prompt...")
            )

            # è·å–æˆ–åˆ›å»ºworkflowå®ä¾‹
            workflow = await self._get_workflow(model_type)

            # æ‰§è¡Œä¼˜åŒ–å·¥ä½œæµ
            logger.info(f"Starting prompt optimization for role: {prompt_request.role}, model: {model_type}")
            result = await workflow.optimize_prompt(prompt_request)

            # æ ¼å¼åŒ–å¹¶å‘é€ç»“æœ
            formatted_result = self._format_result(result)
            event_queue.enqueue_event(new_agent_text_message(formatted_result))
            
            logger.info(f"Prompt optimization completed successfully for role: {prompt_request.role}")

        except ValueError as e:
            await self._send_error_message(event_queue, f"âŒ è¾“å…¥éªŒè¯é”™è¯¯: {str(e)}")
            logger.warning(f"Input validation error: {str(e)}")
        except RuntimeError as e:
            await self._send_error_message(event_queue, f"âŒ ç³»ç»Ÿè¿è¡Œé”™è¯¯: {str(e)}")
            logger.error(f"Runtime error: {str(e)}")
        except Exception as e:
            await self._send_error_message(event_queue, f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°æœªçŸ¥é”™è¯¯: {str(e)}")
            logger.error(f"Unexpected error during execution: {str(e)}", exc_info=True)

    async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:
        """å–æ¶ˆæ‰§è¡Œ"""
        event_queue.enqueue_event(
            new_agent_text_message("âŒ Promptä¼˜åŒ–ä»»åŠ¡å·²å–æ¶ˆ")
        )
        logger.info("Prompt optimization task cancelled")

    async def _get_workflow(self, model_type: str) -> PromptOptimizerWorkflow:
        """è·å–æˆ–åˆ›å»ºworkflowå®ä¾‹ï¼Œä½¿ç”¨ç¼“å­˜æé«˜æ€§èƒ½"""
        if model_type not in self._workflows:
            try:
                self._workflows[model_type] = PromptOptimizerWorkflow(model_type=model_type)
                logger.info(f"Created new workflow for model type: {model_type}")
            except Exception as e:
                logger.error(f"Failed to create workflow for {model_type}: {str(e)}")
                raise RuntimeError(f"æ— æ³•åˆ›å»º {model_type} å·¥ä½œæµ: {str(e)}")
        
        return self._workflows[model_type]

    def _parse_input(self, content: str) -> Optional[Dict[str, Any]]:
        """è§£æç”¨æˆ·è¾“å…¥ï¼Œæ”¯æŒJSONå’Œè‡ªç„¶è¯­è¨€"""
        try:
            # å°è¯•è§£æJSONæ ¼å¼
            parsed_data = json.loads(content)
            logger.info("Successfully parsed JSON input")
            return parsed_data
        except json.JSONDecodeError:
            logger.info("JSON parsing failed, attempting natural language parsing")
            # å¦‚æœä¸æ˜¯JSONï¼Œå°è¯•è§£æè‡ªç„¶è¯­è¨€
            return self._parse_natural_language(content)

    def _parse_natural_language(self, content: str) -> Optional[Dict[str, Any]]:
        """è§£æè‡ªç„¶è¯­è¨€è¾“å…¥ï¼ˆæ”¹è¿›å®ç°ï¼‰"""
        content = content.strip().lower()
        
        # ç®€å•çš„è‡ªç„¶è¯­è¨€è§£æé€»è¾‘
        if any(keyword in content for keyword in ['developer', 'programming', 'code', 'software']):
            return {
                "role": "software developers",
                "examples": [
                    {"input": "Write a function", "output": "def example_function():"},
                    {"input": "Create a class", "output": "class ExampleClass:"}
                ],
                "model_type": "gemini"
            }
        elif any(keyword in content for keyword in ['writer', 'author', 'content', 'writing']):
            return {
                "role": "content writers",
                "examples": [
                    {"input": "Write an article", "output": "Here's a compelling article..."},
                    {"input": "Create a blog post", "output": "Welcome to our blog..."}
                ],
                "model_type": "gemini"
            }
        else:
            logger.info("Could not parse natural language input")
            return None

    def _validate_request_data(self, request_data: Dict[str, Any]) -> Optional[str]:
        """éªŒè¯è¯·æ±‚æ•°æ®çš„å®Œæ•´æ€§å’Œæ­£ç¡®æ€§"""
        if not isinstance(request_data, dict):
            return "è¯·æ±‚æ•°æ®å¿…é¡»æ˜¯JSONå¯¹è±¡æ ¼å¼"
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        if "role" not in request_data:
            return "ç¼ºå°‘å¿…éœ€å­—æ®µ: role"
        
        if "examples" not in request_data:
            return "ç¼ºå°‘å¿…éœ€å­—æ®µ: examples"
        
        # éªŒè¯è§’è‰²å­—æ®µ
        role = request_data.get("role", "")
        if not isinstance(role, str) or not role.strip():
            return "roleå­—æ®µå¿…é¡»æ˜¯éç©ºå­—ç¬¦ä¸²"
        
        # éªŒè¯ç¤ºä¾‹å­—æ®µ
        examples = request_data.get("examples", [])
        if not isinstance(examples, list):
            return "exampleså­—æ®µå¿…é¡»æ˜¯æ•°ç»„"
        
        if len(examples) == 0:
            return "è‡³å°‘éœ€è¦æä¾›ä¸€ä¸ªç¤ºä¾‹"
        
        # éªŒè¯æ¯ä¸ªç¤ºä¾‹
        for i, example in enumerate(examples):
            if not isinstance(example, dict):
                return f"ç¤ºä¾‹ {i+1} å¿…é¡»æ˜¯å¯¹è±¡æ ¼å¼"
            
            if "input" not in example or "output" not in example:
                return f"ç¤ºä¾‹ {i+1} å¿…é¡»åŒ…å« 'input' å’Œ 'output' å­—æ®µ"
            
            if not isinstance(example["input"], str) or not isinstance(example["output"], str):
                return f"ç¤ºä¾‹ {i+1} çš„ 'input' å’Œ 'output' å¿…é¡»æ˜¯å­—ç¬¦ä¸²"
            
            if not example["input"].strip() or not example["output"].strip():
                return f"ç¤ºä¾‹ {i+1} çš„ 'input' å’Œ 'output' ä¸èƒ½ä¸ºç©º"
        
        return None

    async def _send_error_message(self, event_queue: EventQueue, error_message: str) -> None:
        """å‘é€é”™è¯¯æ¶ˆæ¯"""
        event_queue.enqueue_event(new_agent_text_message(error_message))

    def _send_usage_help(self, event_queue: EventQueue):
        """å‘é€ä½¿ç”¨å¸®åŠ©ä¿¡æ¯"""
        help_message = """
ğŸ“‹ **Promptä¼˜åŒ–å™¨ä½¿ç”¨æŒ‡å—**

è¯·æä¾›JSONæ ¼å¼çš„è¾“å…¥ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š

```json
{
    "role": "ç›®æ ‡ç”¨æˆ·è§’è‰²ï¼Œå¦‚ 'software developers', 'book authors', 'customer support reps'",
    "examples": [
        {
            "input": "ç¤ºä¾‹è¾“å…¥1",
            "output": "æœŸæœ›è¾“å‡º1"
        },
        {
            "input": "ç¤ºä¾‹è¾“å…¥2", 
            "output": "æœŸæœ›è¾“å‡º2"
        }
    ],
    "model_type": "æ¨¡å‹ç±»å‹ï¼Œæ”¯æŒ 'gemini'ï¼ˆé»˜è®¤ï¼‰æˆ– 'openai'",
    "additional_requirements": "é¢å¤–è¦æ±‚ï¼ˆå¯é€‰ï¼‰"
}
```

**ğŸ¤– æ”¯æŒçš„æ¨¡å‹ç±»å‹:**
- `gemini`: Google Gemini 2.0 Flash (é»˜è®¤)
- `openai`: OpenAI GPT-4o-mini

**ğŸŒ ä»£ç†é…ç½®:**
ç³»ç»Ÿå·²é…ç½®ä»£ç†æ”¯æŒï¼Œé»˜è®¤ä½¿ç”¨ `http://127.0.0.1:7890`
å¦‚éœ€ä¿®æ”¹ï¼Œè¯·åœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½® `HTTPS_PROXY` å’Œ `HTTP_PROXY`

**ğŸ’¡ å¿«é€Ÿå¼€å§‹ç¤ºä¾‹:**
ç›´æ¥å‘é€ "software developer" æˆ– "content writer" ç­‰å…³é”®è¯ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ç”ŸæˆåŸºç¡€é…ç½®

**ç¤ºä¾‹ï¼šä½¿ç”¨OpenAIæ¨¡å‹ä¼˜åŒ–è½¯ä»¶å¼€å‘prompt**
```json
{
    "role": "software developers",
    "model_type": "openai",
    "examples": [
        {
            "input": "Write a function to calculate fibonacci numbers",
            "output": "def fibonacci(n):\\n    if n <= 1:\\n        return n\\n    return fibonacci(n-1) + fibonacci(n-2)"
        },
        {
            "input": "Create a REST API endpoint",
            "output": "@app.route('/api/users', methods=['GET'])\\ndef get_users():\\n    return jsonify(users)"
        }
    ],
    "additional_requirements": "Focus on clean, maintainable code"
}
```

**ç¤ºä¾‹ï¼šä½¿ç”¨Geminiæ¨¡å‹ä¼˜åŒ–å®¢æœå¯¹è¯prompt**
```json
{
    "role": "customer support representatives",
    "model_type": "gemini",
    "examples": [
        {
            "input": "Customer complains about delayed delivery",
            "output": "I sincerely apologize for the delay. Let me check your order status immediately."
        }
    ]
}
```
        """
        event_queue.enqueue_event(new_agent_text_message(help_message))

    def _format_result(self, result: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–ä¼˜åŒ–ç»“æœï¼Œæ”¹è¿›æ˜¾ç¤ºæ•ˆæœ"""
        model_type = result.get('model_type', 'unknown').upper()
        role = result.get('role', 'Unknown')
        generated_prompt = result.get('generated_prompt', 'N/A')
        evaluations = result.get('evaluations', [])
        alternative_prompts = result.get('alternative_prompts', [])
        final_recommendation = result.get('final_recommendation', generated_prompt)
        original_examples_count = len(result.get('original_examples', []))
        
        formatted = f"""
âœ… **Promptä¼˜åŒ–å®Œæˆ**

ğŸ¯ **ç›®æ ‡ç”¨æˆ·è§’è‰²:** {role}
ğŸ¤– **ä½¿ç”¨æ¨¡å‹:** {model_type}
ğŸ“Š **å¤„ç†ç¤ºä¾‹æ•°é‡:** {original_examples_count}

ğŸ“ **ç”Ÿæˆçš„ä¸»è¦Prompt:**
```
{generated_prompt}
```"""

        # æ·»åŠ è¯„ä¼°ç»“æœ
        if evaluations:
            formatted += f"""

ğŸ” **è¯„ä¼°ç»“æœ:**
{evaluations[0][:500]}{'...' if len(evaluations[0]) > 500 else ''}"""

        # æ·»åŠ æ”¹è¿›æ–¹æ¡ˆ
        if alternative_prompts:
            formatted += f"""

ğŸš€ **æ”¹è¿›æ–¹æ¡ˆ ({len(alternative_prompts)} ä¸ª):**"""
            
            for i, alt_prompt in enumerate(alternative_prompts[:3], 1):  # é™åˆ¶æ˜¾ç¤ºå‰3ä¸ª
                formatted += f"""

**æ–¹æ¡ˆ {i}:**
```
{alt_prompt[:300]}{'...' if len(alt_prompt) > 300 else ''}
```"""

        # æ·»åŠ æœ€ç»ˆæ¨è
        formatted += f"""

ğŸ’¡ **æœ€ç»ˆæ¨è (æ ¹æ®è¯„ä¼°é€‰æ‹©çš„æœ€ä½³æ–¹æ¡ˆ):**
```
{final_recommendation}
```

---
âœ¨ **ä½¿ç”¨å»ºè®®:** æ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨æœ€ç»ˆæ¨èçš„promptï¼Œæˆ–æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©å…¶ä¸­ä¸€ä¸ªæ”¹è¿›æ–¹æ¡ˆã€‚
"""
        
        return formatted 